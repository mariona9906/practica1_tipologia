{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9dfa434",
   "metadata": {},
   "source": [
    "# Practica 2 Tipologia y ciclo de vida de los datos \n",
    "## Dmytro Pravdyvets y Mariona Alberola "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c80e1e",
   "metadata": {},
   "source": [
    "## 1. Descripción del dataset\n",
    "### ¿Por qué es importante y qué pregunta/problema pretende responder?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f871e98",
   "metadata": {},
   "source": [
    "En esta practica vamos a continuar con el dataset generado en la practica anterior. Como hemos expliado previamente, la idea de estas dos practicas es automatizar la busqueda y seleccion de articulos cientificos de interes. \n",
    "\n",
    "Automatizacion de busqueda de infromacion relevante para un investigador es un proceso que puede beneficiar muchisimo a un cientifico ya que le permitira estar mas tiempo haciendo su trabajo que una maquina no puede hacer. \n",
    "\n",
    "En esta practica vamos a mirar cual es el porcentaje de papers que han aplicado ML y AI en el ambito de inmunologia, que algoritmos se han usado y si hay una tendencia de aumento de papers mas computacionales en los ultimos años. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2839602",
   "metadata": {},
   "source": [
    "## 2. Integración y selección de los datos de interés a analizar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ca3fb8",
   "metadata": {},
   "source": [
    "En la anterior practica solo hemos trabajado con los articulos publicados en el año 2022, no obstante hemos pensado que seria mas interesante aumentar este numero y trabajar con papers en los ultimos 5 años (2017-2022) con un total de 549 articulos.\n",
    "\n",
    "Recordemos las columnas de nuestro dataset:\n",
    "\n",
    "*Article*: Nombre del artículo científico.\n",
    "\n",
    "*Summary*: Resumen general de la investigación que se ha llevado a cabo.\n",
    "\n",
    "*Authors*: Nombre de los autores del artículo científico.\n",
    "\n",
    "*Date*: Fecha de publicación.\n",
    "\n",
    "*Access*: Si es de acceso público o se tiene que pagar para acceder al artículo.\n",
    "\n",
    "*Figure*: Una imagen representativa del estudio en cuestión.\n",
    "\n",
    "*Link paper*: Link del artículo científico, los artículos de acceso no público muestran el\n",
    "título del artículo y el resumen general del artículo.\n",
    "\n",
    "*TCR, BCR, T CELL, B CELL, NKC, CD4, CD8, DEEP LEARNING, MACHINE\n",
    "LEARNING ML DL CNN LSTM y HLA*: palabras clave que nos interesan, en caso de que el estudio\n",
    "contenga esa palabra clave el valor va a ser 1 y en caso de no contenerla será 0.\n",
    "\n",
    "\n",
    "De estas columnas, los keywords es la parte mas interesante ya que nos permitira filltrar el data por los diferentes tipos de celulas (T y B cells) y ver cuantos papers ML y AI para hacer el analysis. \n",
    "\n",
    "Una vez tengamos lo papers de interes podemos aplicar NLP para ver la similaritud de los articulos. La idea que tenemos es a traves del abstract crear un heatmap con las distnacias entre los papers de las subcategorias como T-cell ML, B-cell ML por ejemplo para que nos sea mas facil ver papers parecidos a los papers que hemos detectado como papers de interes manualmente. Por ejemplo, hemos sacado todos los papers de interes y hemos leido el primero que va de clasificacion de celulas T CD4+ utilizando ML, con el heatmap podemos ver que otros papers son parecidos a este para no tener que leernos todos los papers de interes, sino solo los mas parecidos al nuestro paper de interes actual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5479fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d04128d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Date</th>\n",
       "      <th>Access</th>\n",
       "      <th>Figure</th>\n",
       "      <th>Link paper</th>\n",
       "      <th>TCR</th>\n",
       "      <th>BCR</th>\n",
       "      <th>T CELL</th>\n",
       "      <th>...</th>\n",
       "      <th>NKC</th>\n",
       "      <th>CD4</th>\n",
       "      <th>CD8</th>\n",
       "      <th>DEEP LEARNING</th>\n",
       "      <th>MACHINE LEARNING</th>\n",
       "      <th>ML</th>\n",
       "      <th>DL</th>\n",
       "      <th>CNN</th>\n",
       "      <th>LSTM</th>\n",
       "      <th>HLA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oxeiptosis, a ROS-induced caspase-independent ...</td>\n",
       "      <td>Reactive oxygen species (ROS) are generated by...</td>\n",
       "      <td>Cathleen HolzeChloé MichaudelAndreas Pichlmair</td>\n",
       "      <td>18 Dec 2017</td>\n",
       "      <td>Payed Access</td>\n",
       "      <td>https://media.springernature.com/w290h158/spri...</td>\n",
       "      <td>https://www.nature.com/articles/s41590-017-0013-y</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Diversification of human plasmacytoid predendr...</td>\n",
       "      <td>Plasmacytoid dendritic cells (pDCs) are known ...</td>\n",
       "      <td>Solana G. AlculumbreViolaine Saint-AndréVassil...</td>\n",
       "      <td>04 Dec 2017</td>\n",
       "      <td>Payed Access</td>\n",
       "      <td>https://media.springernature.com/w290h158/spri...</td>\n",
       "      <td>https://www.nature.com/articles/s41590-017-0012-z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Viral unmasking of cellular 5S rRNA pseudogene...</td>\n",
       "      <td>RIG-I is a cytosolic RNA sensor. Gack and coll...</td>\n",
       "      <td>Jessica J. ChiangKonstantin M. J. SparrerMicha...</td>\n",
       "      <td>27 Nov 2017</td>\n",
       "      <td>Payed Access</td>\n",
       "      <td>https://media.springernature.com/w290h158/spri...</td>\n",
       "      <td>https://www.nature.com/articles/s41590-017-0005-y</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Engagement of MHC class I by the inhibitory re...</td>\n",
       "      <td>Host cells display ‘don’t eat me’ signals to p...</td>\n",
       "      <td>Amira A. BarkalKipp WeiskopfRoy L. Maute</td>\n",
       "      <td>27 Nov 2017</td>\n",
       "      <td>Payed Access</td>\n",
       "      <td>https://media.springernature.com/w290h158/spri...</td>\n",
       "      <td>https://www.nature.com/articles/s41590-017-0004-z</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nuclear RNF2 inhibits interferon function by p...</td>\n",
       "      <td>Cao and colleagues identify the E3 ubiquitin l...</td>\n",
       "      <td>Shuo LiuMinghong JiangXuetao Cao</td>\n",
       "      <td>21 Nov 2017</td>\n",
       "      <td>Payed Access</td>\n",
       "      <td>https://media.springernature.com/w290h158/spri...</td>\n",
       "      <td>https://www.nature.com/articles/s41590-017-0003-0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  \\\n",
       "0  Oxeiptosis, a ROS-induced caspase-independent ...   \n",
       "1  Diversification of human plasmacytoid predendr...   \n",
       "2  Viral unmasking of cellular 5S rRNA pseudogene...   \n",
       "3  Engagement of MHC class I by the inhibitory re...   \n",
       "4  Nuclear RNF2 inhibits interferon function by p...   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  Reactive oxygen species (ROS) are generated by...   \n",
       "1  Plasmacytoid dendritic cells (pDCs) are known ...   \n",
       "2  RIG-I is a cytosolic RNA sensor. Gack and coll...   \n",
       "3  Host cells display ‘don’t eat me’ signals to p...   \n",
       "4  Cao and colleagues identify the E3 ubiquitin l...   \n",
       "\n",
       "                                             Authors         Date  \\\n",
       "0     Cathleen HolzeChloé MichaudelAndreas Pichlmair  18 Dec 2017   \n",
       "1  Solana G. AlculumbreViolaine Saint-AndréVassil...  04 Dec 2017   \n",
       "2  Jessica J. ChiangKonstantin M. J. SparrerMicha...  27 Nov 2017   \n",
       "3           Amira A. BarkalKipp WeiskopfRoy L. Maute  27 Nov 2017   \n",
       "4                   Shuo LiuMinghong JiangXuetao Cao  21 Nov 2017   \n",
       "\n",
       "         Access                                             Figure  \\\n",
       "0  Payed Access  https://media.springernature.com/w290h158/spri...   \n",
       "1  Payed Access  https://media.springernature.com/w290h158/spri...   \n",
       "2  Payed Access  https://media.springernature.com/w290h158/spri...   \n",
       "3  Payed Access  https://media.springernature.com/w290h158/spri...   \n",
       "4  Payed Access  https://media.springernature.com/w290h158/spri...   \n",
       "\n",
       "                                          Link paper  TCR  BCR  T CELL  ...  \\\n",
       "0  https://www.nature.com/articles/s41590-017-0013-y    0    0       0  ...   \n",
       "1  https://www.nature.com/articles/s41590-017-0012-z    0    0       0  ...   \n",
       "2  https://www.nature.com/articles/s41590-017-0005-y    0    0       0  ...   \n",
       "3  https://www.nature.com/articles/s41590-017-0004-z    0    0       0  ...   \n",
       "4  https://www.nature.com/articles/s41590-017-0003-0    0    0       0  ...   \n",
       "\n",
       "   NKC  CD4  CD8  DEEP LEARNING  MACHINE LEARNING  ML  DL  CNN  LSTM  HLA  \n",
       "0    0    0    0              0                 0   0   0    0     0    0  \n",
       "1    0    0    1              0                 0   0   0    0     0    0  \n",
       "2    0    0    0              0                 0   0   0    0     0    0  \n",
       "3    0    1    0              0                 0   0   0    0     0    0  \n",
       "4    0    0    0              0                 0   0   0    0     0    0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"../dataset/2022_immuno_articles.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba28fd89",
   "metadata": {},
   "source": [
    "## 3. Limpieza de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbc0dbb",
   "metadata": {},
   "source": [
    "El primer paso en la limpieza de datos seria verificar que no hay missing data en nuestro dataset. A la hora de diseñar el codigo de web scraping, hemos intentado hacerlo de tal manera que el data sacado de la pagina web sea lo mas completo posible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57e6dfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 549 entries, 0 to 548\n",
      "Data columns (total 21 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Article           549 non-null    object\n",
      " 1   Summary           549 non-null    object\n",
      " 2   Authors           549 non-null    object\n",
      " 3   Date              520 non-null    object\n",
      " 4   Access            549 non-null    object\n",
      " 5   Figure            549 non-null    object\n",
      " 6   Link paper        549 non-null    object\n",
      " 7   TCR               549 non-null    int64 \n",
      " 8   BCR               549 non-null    int64 \n",
      " 9   T CELL            549 non-null    int64 \n",
      " 10  B CELL            549 non-null    int64 \n",
      " 11  NKC               549 non-null    int64 \n",
      " 12  CD4               549 non-null    int64 \n",
      " 13  CD8               549 non-null    int64 \n",
      " 14  DEEP LEARNING     549 non-null    int64 \n",
      " 15  MACHINE LEARNING  549 non-null    int64 \n",
      " 16  ML                549 non-null    int64 \n",
      " 17  DL                549 non-null    int64 \n",
      " 18  CNN               549 non-null    int64 \n",
      " 19  LSTM              549 non-null    int64 \n",
      " 20  HLA               549 non-null    int64 \n",
      "dtypes: int64(14), object(7)\n",
      "memory usage: 90.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dd0483",
   "metadata": {},
   "source": [
    "Podemos ver que no hay missing data ya que todas las columnas tienen el maximo de non-null values. No obstante vemos que las columnas de keywords se identifican como integers, vamos a cambiarlas a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c554e658",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords=[\"TCR\",\"BCR\",\"T CELL\",\"B CELL\",\"NKC\",\"CD4\",\"CD8\",\"DEEP LEARNING\",\"MACHINE LEARNING\",\"ML\",\"DL\",\"CNN\",\"LSTM\",\"HLA\"]\n",
    "for key in keywords:\n",
    "    data[key]=data[key].astype(str)\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e454703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 549 entries, 0 to 548\n",
      "Data columns (total 21 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   Article           549 non-null    object        \n",
      " 1   Summary           549 non-null    object        \n",
      " 2   Authors           549 non-null    object        \n",
      " 3   Date              520 non-null    datetime64[ns]\n",
      " 4   Access            549 non-null    object        \n",
      " 5   Figure            549 non-null    object        \n",
      " 6   Link paper        549 non-null    object        \n",
      " 7   TCR               549 non-null    object        \n",
      " 8   BCR               549 non-null    object        \n",
      " 9   T CELL            549 non-null    object        \n",
      " 10  B CELL            549 non-null    object        \n",
      " 11  NKC               549 non-null    object        \n",
      " 12  CD4               549 non-null    object        \n",
      " 13  CD8               549 non-null    object        \n",
      " 14  DEEP LEARNING     549 non-null    object        \n",
      " 15  MACHINE LEARNING  549 non-null    object        \n",
      " 16  ML                549 non-null    object        \n",
      " 17  DL                549 non-null    object        \n",
      " 18  CNN               549 non-null    object        \n",
      " 19  LSTM              549 non-null    object        \n",
      " 20  HLA               549 non-null    object        \n",
      "dtypes: datetime64[ns](1), object(20)\n",
      "memory usage: 90.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9ac166",
   "metadata": {},
   "source": [
    "Ahora vemos que todas las variables que tenemos no son numericas y la columna Date es de tipo date. Vemos que si a la hora de diseñar el codigo de web scraping se hace un esfuerzo para obtener el data de mayor calidad posible, la limpieza de data sera facil. \n",
    "\n",
    "Dado que no es dataset de data numerico, no tenemos que revisar outliers ni ningun tipo de valores raros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8f82b7",
   "metadata": {},
   "source": [
    "## 4. Análisis de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3ba51b",
   "metadata": {},
   "source": [
    "Como ya hemos dicho previamente, el analisis principal consistira de analisis de similaritud de articulos en funcion del abstract de los articulos. Para ello vamos a necesitar a definir como vamos a calcular la similaritud o la distancia entre los articlos. \n",
    "\n",
    "La primera parte de este problema es la representación. ¿Cómo representamos el texto? Podríamos dejar el texto tal como está o convertirlo en vectores de características utilizando una técnica de incrustación de texto adecuada. Una vez que tenemos la representación del texto, podemos calcular la puntuación de similitud usando una de las muchas medidas de distancia/similitud.\n",
    "\n",
    "## Metricas de distancia \n",
    "\n",
    "### Índice Jaccard\n",
    "\n",
    "El índice de Jaccard, también conocido como coeficiente de similitud de Jaccard, trata los objetos de datos como conjuntos. Se define como el tamaño de la intersección de dos conjuntos dividido por el tamaño de la unión.\n",
    "\n",
    "Para calcular la similitud utilizando la similitud de Jaccard, primero realizaremos la normalización del texto para reducir las raíces/lemas de las palabras.\n",
    "\n",
    "\n",
    "### Distancia euclidiana\n",
    "\n",
    "La distancia euclidiana, o norma L2, es la forma más utilizada de la distancia de Minkowski. En términos generales, cuando la gente habla de distancia, se refiere a la distancia euclidiana. Utiliza el teorema de Pitágoras para calcular la distancia entre dos puntos.\n",
    "\n",
    "Cuanto mayor sea la distancia d entre dos vectores, menor será la puntuación de similitud y viceversa.\n",
    "\n",
    "Las distancias pueden variar de 0 a infinito, necesitamos usar alguna forma de normalizarlas al rango de 0 a 1.\n",
    "\n",
    "Aunque tenemos nuestra fórmula de normalización típica que usa la media y la desviación estándar, es sensible a los valores atípicos. Eso significa que si hay algunas distancias extremadamente grandes, cualquier otra distancia se hará más pequeña como consecuencia de la operación de normalización. Usando la formula: 1/$\\exp$(distance).\n",
    "\n",
    "### Cosine Similarity \n",
    "\n",
    "Cosine Similarity calcula la similitud de dos vectores como el coseno del ángulo entre dos vectores. Determina si dos vectores apuntan aproximadamente en la misma dirección. Entonces, si el ángulo entre los vectores es 0 grados, entonces la similitud del coseno es 1.\n",
    "\n",
    "### ¿Qué métrica usar?\n",
    "\n",
    "La similitud de Jaccard tiene en cuenta solo el conjunto de palabras únicas para cada documento de texto. Esto lo convierte en el candidato probable para evaluar la similitud de los documentos cuando la repetición no es un problema. Un excelente ejemplo de una aplicación de este tipo es comparar descripciones de productos. Por ejemplo, si un término como \"HD\" o \"eficiencia térmica\" se usa varias veces en una descripción y solo una vez en otra, la distancia euclidiana y la similitud del coseno disminuirían. Por otro lado, si el número total de palabras únicas permanece igual, la similitud de Jaccard permanecerá sin cambios.\n",
    "\n",
    "Dicho esto, la similitud de Jaccard no se suele cuando se trabaja con datos de texto, ya que no funciona con embeddings de texto. Esto significa que se limita a evaluar la similitud léxica del texto, es decir, qué tan similares son los documentos a nivel de palabras.\n",
    "\n",
    "En lo que respecta a las métricas de coseno y euclidianas, el factor diferenciador entre las dos es que la similitud del coseno no se ve afectada por la magnitud/longitud de los vectores de características. Digamos que estamos creando un algoritmo de etiquetado de temas. Si una palabra (por ejemplo, senado) aparece con más frecuencia en el documento 1 que en el documento 2, podríamos suponer que el documento 1 está más relacionado con el tema de la política. Sin embargo, también podría darse el caso de que estemos trabajando con artículos de noticias de diferente extensión. Entonces, la palabra 'senado' probablemente apareció más en el documento 1 simplemente porque era mucho más larga. Como vimos anteriormente cuando se repitió la palabra \"vacío\", la similitud del coseno es menos sensible a la diferencia de longitudes.\n",
    "\n",
    "Además de eso, la distancia euclidiana no funciona bien con los vectores dispersos de incrustaciones de texto. Por lo tanto, generalmente se prefiere la similitud del coseno a la distancia euclidiana cuando se trabaja con datos de texto.\n",
    "\n",
    "En el siguiente apartado vamos a explorar como hacer los embeddings de palabras/textos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5ce1fd",
   "metadata": {},
   "source": [
    "## Embeddings de texto\n",
    "\n",
    "Hay mil y una manera de convertir palabras en numeros. Como ejemplo podemos poner One-hot embedding, Term Frequency-Inverse Document Frequency embedding, etc. Sequence embeddings es un ambito en el que tenemos bastante experiencia y podemos decir que la metodologia mas robusta que suele funcionar mejor es algo como Word2Vec. \n",
    "\n",
    "### Word2Vec\n",
    "\n",
    "Word2Vec es un método predictivo para formar embeddings de palabras. Word2Vec es una red neuronal de dos capas preentrenada. Toma como entrada el corpus de texto y genera un conjunto de vectores de características que representan palabras en ese corpus. Esto nos permite generar un vector numerico por palabra que podemos usar para calcular distancias euclidianas y cosenas. \n",
    "\n",
    "No obstante Word2Vec tiene la limitacion de hacer embeddings solo por palabra y esto es una limitacion ya que queremos contextualizar un embedding por todo el abstract. Para esto podemos usar otro modelo como SBET\n",
    "\n",
    "### Sentence-BERT (SBERT)\n",
    "\n",
    "SBERT es una red gemela que le permite procesar dos oraciones de la misma manera, simultáneamente. Estos dos gemelos son idénticos en todos los parámetros (su peso está vinculado), lo que nos permite pensar en esta arquitectura como un solo modelo utilizado varias veces. Es una extension de BERT que esta optimizada para frases en vez de palabras y añade el aspecto de contexto a la hora de hacer los embeddings. Para esta practica vamos a usar SBERT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "118e309d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/dima/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/dima/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/dima/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/dima/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting sentence_transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m442.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m254.7 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/dima/.local/lib/python3.8/site-packages (from sentence_transformers) (4.64.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/dima/.local/lib/python3.8/site-packages (from sentence_transformers) (1.12.1)\n",
      "Requirement already satisfied: torchvision in /home/dima/anaconda3/lib/python3.8/site-packages (from sentence_transformers) (0.12.0)\n",
      "Requirement already satisfied: numpy in /home/dima/.local/lib/python3.8/site-packages (from sentence_transformers) (1.22.4)\n",
      "Requirement already satisfied: scikit-learn in /home/dima/.local/lib/python3.8/site-packages (from sentence_transformers) (1.1.1)\n",
      "Requirement already satisfied: scipy in /home/dima/anaconda3/lib/python3.8/site-packages (from sentence_transformers) (1.8.1)\n",
      "Requirement already satisfied: nltk in /home/dima/anaconda3/lib/python3.8/site-packages (from sentence_transformers) (3.7)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /home/dima/.local/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.2.0)\n",
      "Requirement already satisfied: requests in /home/dima/anaconda3/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\n",
      "Requirement already satisfied: filelock in /home/dima/anaconda3/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/dima/anaconda3/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/dima/anaconda3/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dima/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /home/dima/anaconda3/lib/python3.8/site-packages (from nltk->sentence_transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/dima/.local/lib/python3.8/site-packages (from nltk->sentence_transformers) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/dima/.local/lib/python3.8/site-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
      "Collecting torch>=1.6.0\n",
      "  Using cached torch-1.11.0-cp38-cp38-manylinux1_x86_64.whl (750.6 MB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/dima/.local/lib/python3.8/site-packages (from torchvision->sentence_transformers) (9.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/dima/anaconda3/lib/python3.8/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dima/anaconda3/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/dima/anaconda3/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/dima/anaconda3/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dima/anaconda3/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.5.18.1)\n",
      "Building wheels for collected packages: sentence_transformers\n",
      "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=5d6aca4d51c2d90fd662cdde4485c906f7f8dac8df491e5ed148c8dac0bac665\n",
      "  Stored in directory: /home/dima/.cache/pip/wheels/30/b4/1c/7509ecb4c391a7be4cdf2ff04df077a568cd52471007e436e6\n",
      "Successfully built sentence_transformers\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/dima/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/dima/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tokenizers, sentencepiece, torch, huggingface-hub, transformers, sentence_transformers\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.1\n",
      "    Uninstalling torch-1.12.1:\n",
      "      Successfully uninstalled torch-1.12.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/dima/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/dima/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/dima/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/dima/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/dima/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/dima/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/dima/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/dima/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/dima/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/dima/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/dima/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/dima/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "immuneml 2.2.2 requires h5py<=2.10.0, but you have h5py 3.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface-hub-0.11.1 sentence_transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 torch-1.11.0 transformers-4.25.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/dima/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/dima/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/dima/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/dima/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cipy (/home/dima/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/home/dima/anaconda3/lib/python3.8/site-packages)\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c151c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-30 12:39:18.527143: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-30 12:39:18.578742: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-30 12:39:18.578755: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d485fb26a64ea4a7a8c080bddb9bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/748 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b0997ec5e24398b391f46c22f6c5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a97a444f40f4f83bd50afadb211cbdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3107b3a629447739c043647a2878a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7334316bde4596ad91b6d2b32dd971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/674 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2647310c96d4244b56fd1fcec269ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e6cee6647f4d0db93ea179c48f4c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c32303b5124603a22068f76d02d9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c864b01163b4887be3be002738529a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a721c35279fe4009a9e6080b275b189c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96c100150694dcfa3d3ed93a4e23609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703fb882e48446acb092e35f970b8fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f06ad1613154bc9b35e8855eafb9cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05aaabfcec04caab5189542c42ad743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# embeddings primero\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('stsb-roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0996f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sim(abstracts)\n",
    "    embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "    similarity = []\n",
    "    for i in range(len(sentences)):\n",
    "        row = []\n",
    "        for j in range(len(sentences)):\n",
    "            row.append(util.pytorch_cos_sim(embeddings[i], embeddings[j]).item())\n",
    "        similarity.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4ff76d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c437d25",
   "metadata": {},
   "source": [
    "## 5. Representación de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff29c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18c2e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap(similarity, cmap = \"YlGnBu\"):\n",
    "    df = pd.DataFrame(similarity)\n",
    "    df.columns = labels\n",
    "    df.index = labels\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    sns.heatmap(df, cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d288d84a",
   "metadata": {},
   "source": [
    "## 6. Resolución del problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8f1545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
